{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shruti192/Code_Summarizer/blob/main/Code_summarizer_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementing a sequence to sequence translation architecture to generate summary strings from code snippets.**\n"
      ],
      "metadata": {
        "id": "JU-ACVPiMnz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuK7j4EIDq6X"
      },
      "outputs": [],
      "source": [
        "############################# Data Cleaning and Pre-Processing #########################################\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "############  Function to remove non-alphabetic characters (Data Cleaning)\n",
        "def clean_text(column):\n",
        "    for row in column:\n",
        "        row = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1',  str(row))).split()\n",
        "        row = ' '.join(row)\n",
        "        row = re.sub(\"(\\\\t)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\\\r)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\\\n)\", \" \", str(row)).lower()\n",
        "        # Remove the characters - <>()|&©ø\"',;?~*!\n",
        "        row = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",.\\}`$\\{;@?~*!+=_\\//1234567890]\", \" \", str(row)).lower()\n",
        "        row = re.sub(r\"\\\\b(\\\\w+)(?:\\\\W+\\\\1\\\\b)+\", \"\", str(row)).lower()\n",
        "        # Replace INC nums to INC_NUM\n",
        "        row = re.sub(\"([iI][nN][cC]\\d+)\", \"INC_NUM\", str(row)).lower()\n",
        "        # Replace CM# and CHG# to CM_NUM\n",
        "        row = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", \"CM_NUM\", str(row)).lower()\n",
        "        # Remove punctuations at the end of a word\n",
        "        row = re.sub(\"(\\.\\s+)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\-\\s+)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\:\\s+)\", \" \", str(row)).lower()\n",
        "        # Replace any url to only the domain name\n",
        "        try:\n",
        "            url = re.search(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", str(row))\n",
        "            repl_url = url.group(3)\n",
        "            row = re.sub(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", repl_url, str(row))\n",
        "        except:\n",
        "            pass\n",
        "        row = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1',  str(row))).split()\n",
        "        # Remove multiple spaces\n",
        "        row = re.sub(\"(\\s+)\", \" \", str(row)).lower()\n",
        "        # Remove the single character hanging between any two spaces\n",
        "        row = re.sub(\"(\\s+.\\s+)\", \" \", str(row)).lower()\n",
        "        yield row\n",
        "  \n",
        "\n",
        "df_code = pd.read_csv('/content/drive/MyDrive/NLP/python_Sample_dataset.csv')\n",
        "df_code_p = df_code[[\"code\",\"docstring\"]]\n",
        "\n",
        "processed_code= clean_text(df_code_p['code'])\n",
        "processed_summary = clean_text(df_code_p['docstring'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RJ6rTwoWQfA",
        "outputId": "5047693d-5fb7-4f24-9820-68bfe0cfe7ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "T4N0iOTcJXmx",
        "outputId": "9ba96c31-a111-4bd7-890a-c663180a512b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to clean up everything: 0.14 mins\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "###############  Data Pre-Processing\n",
        "import spacy\n",
        "from time import time\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) \n",
        "t = time()\n",
        "# Process text as batches and yield Doc objects in order\n",
        "code = [str(doc) for doc in nlp.pipe(processed_code, batch_size=50)]\n",
        "summary = [ str(doc)  for doc in nlp.pipe(processed_summary, batch_size=50)]\n",
        "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "df_code_p['code'] = code\n",
        "df_code_p['docstring'] = summary\n",
        "\n",
        "\n",
        "####### Analyzing distribution of code length and summary length\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_count = []\n",
        "summary_count = []\n",
        "\n",
        "for sent in df_code_p['code']:\n",
        "    text_count.append(len(sent.split()))\n",
        "\n",
        "for sent in df_code_p['docstring']:\n",
        "    summary_count.append(len(sent.split()))\n",
        "\n",
        "graph_df = pd.DataFrame() \n",
        "graph_df['text'] = text_count\n",
        "graph_df['summary'] = summary_count\n",
        "graph_df.hist(bins = 10)\n",
        "plt.show()\n",
        "\n",
        "max_code_len = 40\n",
        "max_summary_len = 40\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######    Select the Summaries and Text which fall below max length \n",
        "import numpy as np\n",
        "\n",
        "cleaned_code = np.array(df_code_p['code'])\n",
        "cleaned_summary= np.array(df_code_p['docstring'])\n",
        "short_text = []\n",
        "short_summary = []\n",
        "for i in range(len(cleaned_code)):\n",
        "    if len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_code[i].split()) <= max_code_len:\n",
        "        short_text.append(cleaned_code[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "post_code = pd.DataFrame({'code': short_text,'summary': short_summary})\n",
        "# print(post_code.head(100))\n",
        "print(len(post_code))\n",
        "post_code['summary'] = post_code['summary'].apply(lambda x: 'sostok ' + x         + ' eostok')\n",
        "# print(post_code.head(2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    np.array(post_code[\"code\"]), \n",
        "    np.array(post_code[\"summary\"]),\n",
        "    test_size=0.15,\n",
        "    random_state=0,\n",
        "    shuffle=True,\n",
        ")\n",
        "print(x_train.shape, x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foQwCM5SNRwx",
        "outputId": "33db91ae-2ff2-4e48-8d2d-8f6b2ff1b566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2163\n",
            "(1838,) (325,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vnMhgK4LzUH",
        "outputId": "60d61d63-84e1-4a9d-aecd-3cacf0cde3c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################  tokenization and creation of x vocabolary \n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.utils import tokenize\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ############### Preparing x_vocab or code_vocab\n",
        "# x_tokenizer = Tokenizer() \n",
        "# x_tokenizer.fit_on_texts(list(x_train))\n",
        "# threshold = 1\n",
        "# cnt_infrequent = 0\n",
        "# total_cnt = 0\n",
        "# for key, value in x_tokenizer.word_counts.items():\n",
        "#     total_cnt = total_cnt + 1\n",
        "#     if value < threshold:\n",
        "#         cnt_infrequent = cnt_infrequent + 1 \n",
        "# print(\"% of not frequent words in vocabulary: \", (cnt_infrequent / total_cnt) * 100)\n",
        "\n",
        "# # Prepare a tokenizer, again -- by not considering the rare words\n",
        "# x_tokenizer = Tokenizer(num_words = total_cnt - cnt_infrequent) \n",
        "# x_tokenizer.fit_on_texts(list(x_train))\n",
        "# print(x_tokenizer)\n",
        "\n",
        "# list(tokenize(text))\n",
        "x_tokenized_train=[]\n",
        "print(x_train.shape, x_test.shape)\n",
        "for row in x_train:\n",
        "  row_ = list(tokenize(row))\n",
        "  x_tokenized_train.append(row_)\n",
        "print(len(x_tokenized_train))\n",
        "# train model\n",
        "x_model = gensim.models.Word2Vec(x_tokenized_train, min_count = 1, size = 64, window = 5, sg = 1)\n",
        "# summarize the loaded model\n",
        "\n",
        "x_vocab = list(x_model.wv.vocab)\n",
        "print(x_vocab)\n",
        "# X_train = x_model_train[x_model_train.wv.vocab]\n",
        "# X_train = x_model_train.wv[x_tokenized]\n",
        "X_train = []\n",
        "for tokenized_sentence in x_tokenized_train:\n",
        "  temp_list = []\n",
        "  print(tokenized_sentence)\n",
        "  for word in tokenized_sentence:\n",
        "    word_vector = x_model.wv[word]\n",
        "    temp_list.append(word_vector)\n",
        "  temp_list = np.array(temp_list)\n",
        "  padded = np.zeros((40, 64))\n",
        "  padded[:temp_list.shape[0], :temp_list.shape[1]] = temp_list\n",
        "  X_train.append(temp_list)\n",
        "X_train = np.array(X_train)\n",
        "print(X_train[0].shape)\n",
        "print(X_train[1].shape)\n",
        "print(X_train[10].shape)\n",
        "\n",
        "\n",
        "# print(X_train)\n",
        "x_tokenized_test=[]\n",
        "for row in x_test:\n",
        "  row_ = list(tokenize(row))\n",
        "  x_tokenized_test.append(row_)\n",
        "print(len(x_tokenized_test))\n",
        "\n",
        "x_test_list = []\n",
        "for i in range(len(x_tokenized_test)):\n",
        "  x_preprocessed = \"\"\n",
        "  for word in x_tokenized_test[i]:\n",
        "    if word in x_vocab:\n",
        "      # print(word)\n",
        "      # print('1 : ',x_tokenized_test[i])\n",
        "      # x_tokenized_test[i].remove(word)\n",
        "      # print('2 : ',x_tokenized_test[i])\n",
        "      x_preprocessed += (word + \" \")\n",
        "  x_test_list.append(x_preprocessed)\n",
        "\n",
        "X_test = []\n",
        "for tokenized_sentence in x_tokenized_test:\n",
        "  temp_list = []\n",
        "  for word in tokenized_sentence:\n",
        "    word_vector = x_model.wv[word]\n",
        "    temp_list.append(word_vector)\n",
        "  temp_list = np.array(temp_list)\n",
        "  padded = np.zeros((40, 64))\n",
        "  padded[:temp_list.shape[0], :temp_list.shape[1]] = temp_list\n",
        "  X_test.append(temp_list)\n",
        "X_test = np.array(X_test)\n",
        "print(X_test.shape)\n",
        "\n",
        "# x_voc = x_tokenizer.num_words + 1\n",
        "# print(\"Size of vocabulary in X = {}\".format(x_voc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iJr5wwRQRrGM",
        "outputId": "db920ab1-ca06-403b-afcb-789ec3caa0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1838,) (325,)\n",
            "1838\n",
            "['df', 'fillna', 'method', 'ffill', 'inplace', 'true', 'value', 'for', 'pair', 'in', 'zip', 'a', 'b', 'p', 'wait', 'datetime', 'strptime', 'd', 'm', 'y', 'h', 'all', 'list', 'plot', 'x', 'col', 'name', 'style', 'o', 'str', 'wi', 'wordids', 'sum', 'success', 's', 'join', 'soup', 'find', 'text', 'plt', 'colorbar', 'im', 'ax', 'outfile', 'write', 'n', 'itemlist', 'multiply', 'c', 'axis', 'index', 'levels', 'from', 'subprocess', 'import', 'call', 'astr', 'sys', 'path', 'append', 'to', 'test', 'apple', 'decode', 'iso', 'encode', 'utf', 'isin', 'data', 'frame', 'count', 'groupby', 'city', 'size', 'reset', 'split', 'if', 'not', 'males', 'gender', 'male', 'year', 'i', 'range', 'new', 'copy', 'deepcopy', 'old', 'sorted', 'sort', 'key', 'itemgetter', 'dict', 'canvas', 'delete', 'time', 'strftime', 'open', 'very', 'important', 'txt', 'r', 'read', 'changed', 'int', 'f', 'isdigit', 'else', 'original', 'xc', 'xbc', 'raw', 'unicode', 'escape', 'dummy', 'agg', 'returns', 'np', 'mean', 'scatter', 'color', 'green', 'marker', 'mylist', 're', 'findall', 'z', 'formula', 'print', 'is', 'format', 'foo', 'db', 'collection', 'id', 'false', 'line', 'ignore', 'platform', 'release', 'entry', 'objects', 'filter', 'title', 'exists', 'globals', 'something', 'bob', 'operator', 'weight', 'factor', 'alist', 'os', 'does', 'exist', 'importlib', 'module', 'abc', 'hex', 'v', 'k', 'my', 'items', 'url', 'endswith', 'com', 'root', 'lift', 'position', 'ind', 'enumerate', 'set', 'tuple', 'testdata', 'values', 'bin', 'results', 'union', 'exit', 'j', 'direct', 'output', 'check', 'ls', 'shell', 'item', 'sequence', 'lst', 'ab', 'any', 'equal', 'einsum', 'ij', 'kj', 'jik', 'yourdf', 'drop', 'columnheading', 'urllib', 'request', 'urlretrieve', 'http', 'search', 'twitter', 'json', 'q', 'hi', 'itertools', 'chain', 'listone', 'listtwo', 'pass', 'espeak', 'stderr', 'stdout', 'lambda', 'len', 'reverse', 'exec', 'compile', 'filename', 'py', 'date', 'diff', 'u', 'e', 'fff', 'ipath', 'fn', 'dirname', 'file', 'sub', 'xa', 'ac', 'arg', 'ftp', 'retrbinary', 'retr', 'string', 'food', 'colors', 'abcdabcva', 'strip', 'figure', 'figsize', 'reversed', 'pd', 'merge', 'on', 'how', 'inner', 'ast', 'literal', 'eval', 'yourstring', 'l', 'keys', 'map', 'float', 'max', 'score', 'month', 'current', 'level', 'or', 'g', 'cards', 'car', 'of', 'manufacture', 'listofdict', 'shutil', 'rmtree', 'errors', 'onerror', 'none', 'instance', 'class', 'savez', 'tmp', 'getarray', 'iterable', 'mystring', 'peak', 'weeks', 'ascending', 'match', 'off', 'delicious', 'ham', 'baz', 'foos', 'bar', 'w', 'doesn', 't', 'this', 'it', 'technically', 'works', 'select', 'div', 'dbb', 'commit', 'elements', 'counter', 'dicts', 'system', 'venv', 'python', 'myscript', 'networkx', 'draw', 'labels', 'pos', 'pickle', 'dump', 'some', 'sz', 'func', 'params', 'start', 'random', 'choice', 'shape', 'replace', 'next', 'source', 'bashrc', 'shopt', 'expand', 'aliases', 'nuke', 'script', 'imtag', 'img', 'group', 'abcd', 'yth', 'ytwec', 'lists', 'ex', 'xe', 'mple', 'upper', 'bb', 'array', 'pandas', 'concat', 'duplicates', 'sdfasdf', 'spam', 'elem', 'tag', 'iter', 'connection', 'send', 'established', 'input', 'press', 'enter', 'continue', 'abcdefg', 'cursor', 'execute', 'sql', 'theset', 'lower', 'thedict', 'the', 'grouped', 'session', 'page', 'environ', 'user', 'std', 'fruit', 'unstack', 'rstrip', 'webbrowser', 'example', 'ji', 'parse', 'unquote', 'na', 'monthrange', 'stats', 'astype', 'abspath', 'dogtail', 'rawinput', 'click', 'ext', 'dir', 'bool', 'utcnow', 'isoformat', 'stock', 'same', 'as', 'positions', 'no', 'here', 'found', 'woduplicates', 'lseperated', 'orblist', 'ffff', 'yscale', 'log', 'aa', 'br', 'form', 'nr', 'ctypes', 'windll', 'message', 'box', 'error', 'gmtime', 'getmtime', 'issubset', 'del', 'mykey', 'apply', 'tolist', 'chr', 'myintegers', 'gdb', 'slice', 'arr', 'instancelist', 'hello', 'markus', 'difflib', 'matcher', 'aaabcabccd', 'total', 'screencapture', 'screen', 'png', 'updated', 'dumps', 'your', 'ensure', 'ascii', 'content', 'driver', 'element', 'by', 'css', 'selector', 'button', 'logging', 'get', 'logger', 'debug', 'model', 'regex', 'shift', 'cumsum', 'writer', 'writerow', 'first', 'args', 'firstname', 'gca', 'invert', 'xaxis', 'xff', 'newname', 'translate', 'punctuation', 'xor', 'mv', 'home', 'somedir', 'subdir', 'imp', 'fromlist', 'val', 'stdin', 'timedelta', 'hours', 'doc', 'xpath', 'starts', 'with', 'min', 'inf', 'math', 'isnan', 'monthly', 'nonzero', 'raise', 'represents', 'hidden', 'bug', 'do', 'catch', 'permutations', 'csv', 'cols', 'writeheader', 'integers', 'isinstance', 'fields', 'required', 'para', 'bc', 'aabcc', 'dotall', 'islice', 'def', 'return', 'mask', 'intersection', 'loc', 'one', 'three', 'onlyfiles', 'listdir', 'mypath', 'isfile', 'linspace', 'num', 'endpoint', 'ts', 'substring', 'dparser', 'monkey', 'love', 'banana', 'fuzzy', 'yourdata', 'subkey', 'insert', 'into', 'rank', 'aeiou', 'ord', 'andr', 'realpath', 'trx', 'row', 'mytuple', 'www', 'org', 'owl', 'fileinput', 'mac', 'eol', 'quit', 'requests', 'payload', 'rename', 'last', 'rsplit', 'legend', 'numpoints', 'li', 'ix', 'chdir', 'fire', 'shot', 'light', 'sin', 'an', 'function', 'etc', 'password', 'dropna', 'subset', 'four', 'five', 'now', 'age', 'thisismylist', 'pyplot', 'tadas', 'word', 'lines', 'xydata', 'addr', 'sep', 'names', 'region', 'amodule', 'sleep', 'letter', 'number', 'writerows', 'profile', 'reputation', 'markup', 'lol', 'selected', 'queryresult', 'exception', 'you', 'expect', 'handle', 'load', 'product', 'repeat', 'corrcoef', 'sample', 'words', 'startswith', 'hash', 'pformat', 'lstrip', 'round', 'significant', 'digit', 'add', 'plain', 'numpy', 'where', 'urlopen', 'stackoverflow', 'getcode', 'setdefaultencoding', 'tex', 'end', 'struct', 'pack', 'trace', 'ratio', 'java', 'jar', 'blender', 'column', 'elems', 'today', 'mike', 'that', 'might', 'minute', 'along', 'linalg', 'norm', 'nonposy', 'clip', 'minutes', 'username', 'par', 'try', 'mydict', 'except', 'mynewlist', 'myset', 'extra', 'char', 'length', 'weekly', 'visitors', 'daily', 'randint', 'xlabel', 'temperature', 'dog', 'google', 'strings', 'getall', 'matplotlib', 'rc', 'update', 'font', 'alpha', 'savefig', 'bbox', 'inches', 'tight', 'fontsize', 'small', 'response', 'type', 'application', 'groups', 'window', 'scroll', 'document', 'body', 'height', 'psutil', 'cpu', 'percent', 'virtual', 'memory', 'fname', 'readlines', 'powerset', 'awk', 'query', 'marley', 'conversions', 'trials', 'popen', 'rm', 'xd', 'parser', 'argument', 'conf', 'nargs', 'action', 'walls', 'stat', 'lib', 'genericpath', 'st', 'columns', 'prefix', 'glob', 'adam', 'result', 'engine', 'xs', 'namelist', 'thelist', 'thefile', 'mycsv', 'nan', 'isnull', 'checkmark', 'obj', 'savetxt', 'loads', 'sig', 'component', 'report', 'dji', 'xls', 'rb', 'gold', 'geometry', 'avg', 'calendar', 'myfunction', 'km', 'fit', 'reshape', 'pivot', 'table', 'rows', 'grouper', 'dirpath', 'dirnames', 'filenames', 'walk', 'extend', 'break', 'io', 'encoding', 'le', 'floats', 'browser', 'removedirs', 'reindex', 'like', 'dct', 'order', 'option', 'display', 'distinct', 'choices', 'onclick', 'bedroom', 'deluxe', 'geoloc', 'country', 'workbook', 'xlsxwriter', 'users', 'steven', 'documents', 'demo', 'xlsx', 'runtime', 'specific', 'html', 'wx', 'ctrl', 'self', 'xxx', 'width', 'px', 'arange', 'nd', 'urlfetch', 'fetch', 'deadline', 'app', 'run', 'variable', 'seq', 'day', 'january', 'adfix', 'diag', 'rot', 'isalpha', 'worth', 'more', 'than', 'sublist', 'con', 'msbuild', 'project', 'sln', 'configuration', 'datafile', 'blabla', 'quote', 'plus', 'characters', 'these', 'ok', 'feature', 'ch', 'unicodedata', 'category', 'archive', 'pdffile', 'basename', 'super', 'executive', 'init', 'vars', 'sqrt', 'usr', 'perl', 'uireplace', 'pl', 'var', 'bis', 'frameon', 'lestring', 'lelist', 'pdf', 'reference', 'base', 'href', 'javascript', 'windows', 'forked', 'pdb', 'jsobj', 'dtypes', 'object', 'assertion', 'unexpected', 'distance', 'argwhere', 'somestring', 'xdeadbeef', 'storbinary', 'stor', 'myfile', 'and', 'param', 'normalize', 'nfkd', 'xfasica', 'zipped', 'https', 'mysite', 'auth', 'pwd', 'conv', 'nvalues', 'zfill', 'wget', 'download', 'times', 'happy', 'hats', 'cats', 'mmap', 'fileno', 'access', 'csvwriter', 'places', 'post', 'code', 'maketrans', 'httpbin', 'beginning', 'partition', 'getattr', 'builtins', 'counts', 'accidents', 'template', 'view', 'unpack', 'child', 'node', 'svg', 'train', 'rel', 'know', 'chapter', 'tuples', 'tup', 'two', 'iloc', 'ppl', 'room', 'cumcount', 'hashtags', 'sh', 'otherfunc', 'iwashere', 'appended', 'setdefault', 'somekey', 'emaillist', 'jvm', 'dappdynamics', 'dsomeotherparam', 'finditer', 'outer', 'info', 'myapp', 'linestyle', 'wsservice', 'pkl', 'wb', 'out', 'asking', 'cluster', 'blah', 'gdp', 'persons', 'passport', 'birth', 'container', 'dictionary', 'grep', 'pipe', 'communicate', 'ntwo', 'nthree', 'nfour', 'nfive', 'nsix', 'command', 'another', 'saleid', 'upc', 'aggfunc', 'fill', 'mydata', 'cat', 'flush', 'bull', 'files', 'jpg', 'somelist', 'image', 'show', 'host', 'port', 'binascii', 'property', 'default', 'tcsh', 'own', 'final', 'close', 'executable', 'pprint', 'ddd', 'aaa', 'bbb', 'ccc', 'eee', 'contains', 'strs', 'zeros', 'keywords', 'parent', 'series', 'stngs', 'buckets', 'hasattr', 'temp', 'tt', 'abcdef', 'push', 'sheet', 'background', 'red', 'splitlines', 'winsound', 'play', 'sound', 'wav', 'snd', 'concatenate', 'ravel', 'hdf', 'pop', 'grid', 'bcd', 'ef', 'quadmesh', 'clim', 'vmin', 'vmax', 'mydf', 'tsv', 'linsolve', 'matrix', 'urls', 'eu', 'full', 'rfind', 'api', 'randomkey', 'xyz', 'cleaned', 'xml', 'transform', 'keyvalues', 'ips', 'smth', 'expenses', 'loadtxt', 'delimiter', 'skiprows', 'idxmax', 'american', 'mp', 'avi', 'remove', 'flt', 'article', 'pub', 'annotate', 'relativedelta', 'months', 'commonprefix', 'taa', 'atgc', 'shutdown', 'unsorted', 'ff', 'somedirectory', 'gtk', 'center', 'newstr', 'oldstr', 'custom', 'pk', 'meta', 'are', 'welcome', 'john', 'echo', 'world', 'attr', 'label', 'top', 'left', 'right', 'collections', 'most', 'common', 'isupper', 'sex', 'female', 'bf', 'fda', 'needle', 'haystack', 'bigdict', 'formatter', 'asctime', 'levelname', 'mydic', 'fromstring', 'dtype', 'treeview', 'connect', 'allocate', 'days', 'dstack', 'meshgrid', 'needed', 'argumet', 'go', 'fromfile', 'done', 'el', 'put', 'headers', 'someotherkey', 'somekeyggg', 'de', 'fg', 'again', 'multiline', 'unhexlify', 'dt', 'jun', 'pm', 'dic', 'mktime', 'timetuple', 'microsecond', 'season', 'ordered', 'fromkeys', 'simplejson', 'koko', 'fig', 'subplots', 'adjust', 'wspace', 'hspace', 'ld', 'grepdb', 'bash', 'picture', 'yaxis', 'markdown', 'pdfkit', 'head', 'tail', 'dateobj', 'datestr', 'anniversary', 'vstack', 'create', 'purisa', 'cur', 'mogrify', 'yaml', 'stream', 'exc', 'bad', 'thing', 'happened', 'comma', 'quotechar', 'res', 'status', 'newcontents', 'contents', 'employees', 'eng', 'flat', 'yticklabels', 'package', 'attrgetter', 'مپ', 'submit', 'attribute', 'excel', 'exe', 'lo', 'cla', 'section', 'bo', 'men', 'rocks', 'mountains', 'please', 'muffin', 'lolz', 'kitty', 'attrb', 'bcdfghjklmnpqrstvwxyz', 'concertation', 'ignorecase', 'quarter', 'uname', 'desktop', 'config', 'state', 'disabled', 'cp', 'link', 'combine', 'dateobject', 'ampersand', 'apostrophe', 'subdirname', 'idx', 'fly', 'pid', 'getpid', 'process', 'use', 'getcwd', 'modules', 'ins', 'distances', 'dates', 'just', 'applymap', 'sentence', 'factorize', 'td', 'tr', 'directory', 'uri', 'authorization', 'tok', 'token', 'bufcount', 'buf', 'while', 'genfromtxt', 'perc', 'goog', 'aapl', 'conn', 'httplib', 'getresponse', 'reason', 'functools', 'reduce', 'xab', 'xbb', 'pattern', 'testfile', 'ur', 'lopener', 'retrieve', 'randomsite', 'gz', 'duck', 'codecs', 'tick', 'prop', 'resample', 'setp', 'xx', 'ticks', 'include', 'sms', 'implicitly', 'records', 'hashlib', 'md', 'hexdigest', 'makeitastring', 'kg', 'lb', 'gal', 'employee', 'elevations', 'actions', 'fixed', 'expanduser', 'ini', 'cents', 'dollars', 'bread', 'sale', 'folders', 'rootdir', 'polishpottery', 'headfirstpython', 'help', 'pylab', 'ylim', 'congress', 'pluto', 'getenv', 'ma', 'tile', 'cond', 'argmax', 'textfile', 'fh', 'seek', 'objs', 'there', 'vf', 'models', 'foreign', 'unique', 'ceil', 'abs', 'urlparse', 'urldefrag', 'address', 'condition', 'comments', 'tom', 'jerry', 'mouse', 'spark', 'stuff', 'attributes', 'topmost', 'comment', 'rdata', 'race', 'track', 'pygame', 'mode', 'fullscreen', 'brand', 'etree', 'tostring', 'strong', 'destroy', 'colour', 'kind', 'students', 'marks', 'beautiful', 'book', 'functions', 'cwd', 'locale', 'setlocale', 'lc', 'en', 'us', 'grouping', 'sad', 'people', 'cls', 'clear', 'flags', 'instructor', 'parsed', 'attrs', 'hand', 'thecakeisalie', 'aaabbbccc', 'argsort', 'ffffffbbbbbbbqqq', 'xticks', 'cool', 'beans', 'isocalendar', 'joe', 'blow', 'cheers', 'master', 'he', 'llo', 'fromtimestamp', 'domid', 'description', 'proc', 'second', 'member', 'aloha', 'cmd', 'version', 'superset', 'boat', 'fisher', 'folder', 'solve', 'dot', 'combinations', 'cdf', 'jan', 'sites', 'relpath', 'od', 'capital', 'decimal', 'randrange', 'strg', 'transpose', 'jobs', 'luvz', 'me', 'periods', 'freq', 'bm', 'server', 'smtplib', 'smtp', 'gmail', 'grade', 'focus', 'site', 'superuser', 'sdkjh', 'asd', 'sd', 'referer', 'cost', 'amount', 'rs', 'nworld', 'main', 'arrays', 'somefile', 'userprofile', 'gte', 'executemany', 'alkaline', 'earth', 'argparse', 'allow', 'abbrev', 'nodes', 'aabb', 'pixels', 'lowercase', 'follow', 'pear', 'fish', 'fd', 'wronly', 'creat', 'excl', 'birthdays', 'stru', 'dummies', 'stack', 'square', 'frozenset', 'tar', 'thread', 'review', 'somesite', 'adminpanel', 'php', 'cr', 'block', 'dim', 'pat', 'chmod', 'irusr', 'irgrp', 'iroth', 'gzip', 'rt', 'parts', 'presorted', 'destination', 'terminate', 'has', 'locals', 'desired', 'dropwhile', 'chars', 'trial', 'foobar', 'sparse', 'rmdir', 'textblock', 'zone', 'europe', 'istanbul', 'logical', 'op', 'ell', 'orld', 'copyfile', 'src', 'dst', 'phrase', 'nth', 'header', 'dirs', 'cmp', 'things', 'getlist', 'unix', 'adict', 'messages', 'tree', 'setattr', 'retval', 'authenticate', 'basic', 'realm', 'longlong', 'tes', 'tstring', 'xb', 'xba', 'latin', 'ud', 'ude', 'surrogatepass']\n",
            "['df', 'fillna', 'method', 'ffill', 'inplace', 'true']\n",
            "['value', 'for', 'pair', 'in', 'zip', 'a', 'b', 'for', 'value', 'in', 'pair']\n",
            "['p', 'wait']\n",
            "['datetime', 'strptime', 'd', 'm', 'y', 'h', 'm']\n",
            "['all', 'a', 'list']\n",
            "['zip', 'list', 'a', 'list', 'b']\n",
            "['df', 'plot', 'x', 'col', 'name', 'y', 'col', 'name', 'style', 'o']\n",
            "['str', 'wi', 'for', 'wi', 'in', 'wordids']\n",
            "['sum', 'd', 'success', 'for', 'd', 'in', 's']\n",
            "['join', 'soup', 'find', 'all', 'text', 'true']\n",
            "['plt', 'colorbar', 'im', 'ax', 'ax']\n",
            "['outfile', 'write', 'n', 'join', 'itemlist']\n",
            "['df', 'a', 'b', 'multiply', 'df', 'c', 'axis', 'index']\n",
            "['levels']\n",
            "['from', 'subprocess', 'import', 'call']\n",
            "['astr']\n",
            "['sys', 'path', 'append', 'path', 'to', 'test']\n",
            "['apple', 'decode', 'iso', 'encode', 'utf']\n",
            "['df', 'df', 'a', 'isin']\n",
            "['data', 'frame', 'count', 'df', 'groupby', 'name', 'city', 'size', 'reset', 'index']\n",
            "['s', 'split']\n",
            "['x', 'for', 'x', 'in', 'a', 'if', 'x', 'not', 'in']\n",
            "['males', 'df', 'df', 'gender', 'male', 'df', 'year']\n",
            "['i', 'for', 'i', 'in', 'range', 'if', 'i', 'if', 'i']\n",
            "['new', 'list', 'copy', 'deepcopy', 'old', 'list']\n",
            "['sorted', 'list', 'sorted', 'list', 'to', 'sort', 'key', 'itemgetter']\n",
            "['dict', 'i', 'i', 'for', 'i', 'in', 'range']\n",
            "['canvas', 'delete', 'all']\n",
            "['time', 'strftime', 'y', 'm', 'd', 'h', 'm']\n",
            "['str', 'open', 'very', 'important', 'txt', 'r', 'read']\n",
            "['changed', 'list', 'int', 'f', 'if', 'f', 'isdigit', 'else', 'f', 'for', 'f', 'in', 'original', 'list']\n",
            "['x', 'xc', 'xbc', 'y', 'xc', 'x', 'f', 'encode', 'raw', 'unicode', 'escape', 'decode', 'utf']\n",
            "['df', 'groupby', 'dummy', 'agg', 'returns', 'np', 'mean', 'np', 'sum']\n",
            "['new', 'list', 'x', 'split', 'for', 'x', 'in', 'original', 'list']\n",
            "['scatter', 'x', 'y', 's', 'color', 'green', 'marker', 'h']\n",
            "['mylist', 'sort']\n",
            "['re', 'findall', 'b', 'a', 'z', 'formula']\n",
            "['print', 'value', 'is', 'format', 'value']\n",
            "['foo']\n",
            "['db', 'collection', 'find', 'id', 'false']\n",
            "['line', 'line', 'decode', 'utf', 'ignore', 'encode', 'utf']\n",
            "['import', 'platform', 'platform', 'release']\n",
            "['entry', 'objects', 'filter', 'name', 'name', 'title', 'title', 'exists']\n",
            "['globals', 'something', 'bob']\n",
            "['mylist', 'sort', 'key', 'operator', 'itemgetter', 'weight', 'factor']\n",
            "['alist']\n",
            "['print', 'os', 'path', 'exists', 'does', 'not', 'exist']\n",
            "['importlib', 'import', 'module', 'a', 'b', 'c']\n",
            "['x', 'abc', 'encode', 'hex']\n",
            "['x', 'x', 'for', 'x', 'x', 'in', 'zip', 'list', 'list']\n",
            "['dict', 'v', 'k', 'for', 'k', 'v', 'in', 'my', 'dict', 'items']\n",
            "['if', 'url', 'endswith', 'com', 'url', 'url']\n",
            "['root', 'lift']\n",
            "['p', 'id', 'id', 'p', 'id', 'position', 'ind', 'for', 'ind', 'p', 'in', 'enumerate', 'p', 'list']\n",
            "['list', 'i', 'for', 'i', 'in', 'set', 'tuple', 'i', 'for', 'i', 'in', 'testdata']\n",
            "['sum', 'd', 'values']\n",
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d6a3feefba95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mtemp_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0mpadded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtemp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mtemp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################  tokenization and creation of x vocabolary \n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.utils import tokenize\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ############### Preparing x_vocab or code_vocab\n",
        "# x_tokenizer = Tokenizer() \n",
        "# x_tokenizer.fit_on_texts(list(x_train))\n",
        "# threshold = 1\n",
        "# cnt_infrequent = 0\n",
        "# total_cnt = 0\n",
        "# for key, value in x_tokenizer.word_counts.items():\n",
        "#     total_cnt = total_cnt + 1\n",
        "#     if value < threshold:\n",
        "#         cnt_infrequent = cnt_infrequent + 1 \n",
        "# print(\"% of not frequent words in vocabulary: \", (cnt_infrequent / total_cnt) * 100)\n",
        "\n",
        "# # Prepare a tokenizer, again -- by not considering the rare words\n",
        "# x_tokenizer = Tokenizer(num_words = total_cnt - cnt_infrequent) \n",
        "# x_tokenizer.fit_on_texts(list(x_train))\n",
        "# print(x_tokenizer)\n",
        "\n",
        "# list(tokenize(text))\n",
        "y_tokenized_train=[]\n",
        "for row in y_train:\n",
        "  row_ = list(tokenize(row))\n",
        "  y_tokenized_train.append(row_)\n",
        "\n",
        "# train model\n",
        "y_model = gensim.models.Word2Vec(y_tokenized_train, min_count = 1, size = 64, window = 5, sg = 1)\n",
        "# summarize the loaded model\n",
        "\n",
        "y_vocab = list(y_model.wv.vocab)\n",
        "# X_train = x_model_train[x_model_train.wv.vocab]\n",
        "# X_train = x_model_train.wv[x_tokenized]\n",
        "Y_train = []\n",
        "for tokenized_sentence in y_tokenized_train:\n",
        "  temp_list = []\n",
        "  for word in tokenized_sentence:\n",
        "    word_vector = y_model.wv[word]\n",
        "    temp_list.append(word_vector)\n",
        "  temp_list = np.array(temp_list)\n",
        "  Y_train.append(temp_list)\n",
        "Y_train = np.array(Y_train)\n",
        "print(Y_train.shape)\n",
        "\n",
        "# print(X_train)\n",
        "y_tokenized_test=[]\n",
        "for row in y_test:\n",
        "  row_ = list(tokenize(row))\n",
        "  y_tokenized_test.append(row_)\n",
        "\n",
        "y_test_list = []\n",
        "for i in range(len(y_tokenized_test)):\n",
        "  y_preprocessed = \"\"\n",
        "  for word in y_tokenized_test[i]:\n",
        "    if word in y_vocab:\n",
        "      # print(word)\n",
        "      # print('1 : ',x_tokenized_test[i])\n",
        "      # x_tokenized_test[i].remove(word)\n",
        "      # print('2 : ',x_tokenized_test[i])\n",
        "      y_preprocessed += (word + \" \")\n",
        "  y_test_list.append(y_preprocessed)\n",
        "\n",
        "y_tokenized_test=[]\n",
        "for row in y_test_list:\n",
        "  row_ = list(tokenize(row))\n",
        "  y_tokenized_test.append(row_)\n",
        "\n",
        "Y_test = []\n",
        "for tokenized_sentence in y_tokenized_test:\n",
        "  temp_list = []\n",
        "  for word in tokenized_sentence:\n",
        "    word_vector = y_model.wv[word]\n",
        "    temp_list.append(word_vector)\n",
        "  temp_list = np.array(temp_list)\n",
        "  Y_test.append(temp_list)\n",
        "Y_test = np.array(Y_test)\n",
        "print(Y_test.shape)\n",
        "\n",
        "# x_voc = x_tokenizer.num_words + 1\n",
        "# print(\"Size of vocabulary in X = {}\".format(x_voc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42fd3b0a-3d18-4edc-bb44-0f6bdf38cdd5",
        "id": "TG5l2rd6bEUS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1833,)\n",
            "(324,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:84: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############### LSTM based Encoder-Decoder Architecture and Training  ####################################\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim = 200\n",
        "X_train = tensorflow.convert_to_tensor(X_train, dtype=tensorflow.float32)\n",
        "Y_train = tensorflow.convert_to_tensor(Y_train, dtype=tensorflow.float32)\n",
        "X_test = tensorflow.convert_to_tensor(X_test, dtype=tensorflow.float32)\n",
        "Y_test = tensorflow.convert_to_tensor(Y_test, dtype=tensorflow.float32)\n",
        "#######  Encoder Architecture ####################################################\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_code_len, ))\n",
        "# Embedding layer\n",
        "# enc_emb = Embedding(x_voc, embedding_dim,\n",
        "# trainable=True)(encoder_inputs)\n",
        "# Encoder LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
        "return_state=True, dropout=0.4,\n",
        "recurrent_dropout=0.4)\n",
        "(encoder_output1, state_h1, state_c1) = encoder_lstm1(X_train)\n",
        "# Encoder LSTM 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
        "return_state=True, dropout=0.4,\n",
        "recurrent_dropout=0.4)\n",
        "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
        "# Encoder LSTM 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
        "return_sequences=True, dropout=0.4,\n",
        "recurrent_dropout=0.4)\n",
        "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
        "\n",
        "\n",
        "########### Set up the decoder, using encoder_states as the initial state\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "# # Embedding layer\n",
        "# dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
        "# dec_emb = dec_emb_layer(decoder_inputs)\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
        "return_state=True, dropout=0.4,\n",
        "recurrent_dropout=0.2)\n",
        "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
        "decoder_lstm(Y_train, initial_state=[state_h, state_c])\n",
        "# Dense layer\n",
        "y_voc = len(y_model.wv.vocab)\n",
        "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
        "\n",
        "print(\"########### 1 (80, 128)============================================================\")\n",
        "history1 = model.fit(\n",
        "    [X_train, Y_train[:, :-1]],\n",
        "    Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1)[:, 1:],\n",
        "    epochs=80,\n",
        "    callbacks=[es],\n",
        "    batch_size=128,\n",
        "    # validation_data=([X_test, Y_test[:, :-1]],Y_test.reshape(Y_test.shape[0], Y_test.shape[1], 1)[:, 1:]),\n",
        "    )\n",
        "\n",
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.plot(history1.history['loss'], label='train')\n",
        "pyplot.plot(history1.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "\n",
        "\n",
        "reverse_target_word_index = y_tokenizer.index_word\n",
        "reverse_source_word_index = x_tokenizer.index_word\n",
        "target_word_index = y_tokenizer.word_index\n",
        "\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "####### Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
        "decoder_hidden_state_input = Input(shape=(max_code_len, latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
        "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
        "                      decoder_state_input_h, decoder_state_input_c],\n",
        "                      [decoder_outputs2] + [state_h2, state_c2])\n",
        "\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        (output_tokens, h, c) = decoder_model.predict([target_seq]+ [e_out, e_h, e_c])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        if sampled_token != 'eostok':\n",
        "            decoded_sentence += ' ' + sampled_token\n",
        "        # Exit condition: either hit max length or find the stop word.\n",
        "        if sampled_token == 'eostok' or len(decoded_sentence.split()) >= max_summary_len - 1:\n",
        "            stop_condition = True\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        # Update internal states\n",
        "        (e_h, e_c) = (h, c)\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "\n",
        "def seq2summary(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0 and i != target_word_index['sostok'] and i != target_word_index['eostok']:\n",
        "            newString = newString + reverse_target_word_index[i] + ' '\n",
        "    return newString\n",
        "\n",
        "\n",
        "######### To convert sequence to text\n",
        "def seq2text(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0:\n",
        "            newString = newString + reverse_source_word_index[i] + ' '\n",
        "    return newString\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "sum = 0\n",
        "for i in range(0, 75):\n",
        "    print ('Code:', seq2text(x_train[i]))\n",
        "    reference = seq2summary(y_train[i])\n",
        "    candidate = decode_sequence(x_train[i].reshape(1, max_code_len))\n",
        "    print ('Original summary:', reference)\n",
        "    print ('Predicted summary:', candidate)\n",
        "    score = sentence_bleu(reference, candidate)\n",
        "    sum += score\n",
        "    print('BLEU Score:', score)\n",
        "    print ('\\n')\n",
        "\n",
        "sum /= y_train.shape[0]\n",
        "print('avg_BLEU_Score: ', sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "XkVbsq_2lxrn",
        "outputId": "a791bab5-4dfb-445b-8c97-93e63be56912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a288b6f77461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlatent_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R11r-i0qK1gI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Code_summarizer_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_Summarizer_V0.ipynb",
      "provenance": [],
      "mount_file_id": "1ksXSYxYSebH1MrblxDnBANTGf-r9QW_X",
      "authorship_tag": "ABX9TyOZ7PaOJUNK8K2jRHeQ1yvl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shruti192/Code_Summarizer/blob/main/code_Summarizer_V0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DojpaBAzcuVK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe5644bc-c503-44f5-fb5d-14ada704d69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to clean up everything: 0.27 mins\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of not frequent words in vocabulary:  45.80934101087652\n",
            "[[695, 696, 395, 68], [28, 8, 4, 8, 3, 9, 17, 8], [77, 42, 4, 42, 3, 6, 134, 27, 48], [], [10, 42, 4, 10, 3, 37, 17, 42, 3, 10], [522, 25], [6, 99, 290, 6, 10], [93, 114, 471, 523], [13, 78, 71, 123], [150, 6, 2, 4, 2, 3, 16, 6], [5, 39, 3, 10, 5, 14], [26, 6, 305, 495, 23, 11, 18, 2, 34, 2, 119, 35], [26, 5, 11, 18, 2, 33, 2, 2, 119, 35], [181, 29], [13, 12, 50, 1, 1, 1, 73], [12, 25, 1, 253, 49, 1, 58, 9], [26, 75, 11, 18, 2, 2, 2], [44, 6, 53, 11, 18, 2, 2], [7, 187, 29, 229, 35], [9, 69, 60, 161], [166, 39, 5, 77], [26, 542], [13, 125, 305, 59], [57, 79, 22, 7, 90, 24], [30, 4, 30, 3, 5, 17, 3, 30], [113, 282, 6, 47], [2, 4, 2, 3, 16, 6, 17, 67, 2, 801, 1, 1], [7, 7, 187, 29], [7, 309, 229, 35], [524, 524, 90, 100, 211, 100], [184, 109, 343], [24, 28, 24, 84, 4, 84, 3, 6], [4, 8, 3, 47, 70], [175], [233, 89, 5, 32, 25], [185, 76, 123, 88], [40, 8, 40, 8, 40, 8, 4, 8, 3, 47, 45, 40], [57, 327, 5, 14, 329, 5, 14, 260, 705], [111, 145, 748, 148, 43, 3, 204, 204, 294], [6, 38, 30, 4, 30, 3, 37], [7, 71, 39], [11, 42, 11, 4, 11, 3, 712], [2, 42, 4, 2, 3, 6, 138], [22, 22, 106, 1, 1, 137, 147, 36, 63], [5, 39, 113, 215, 5, 245, 71, 88], [26, 11, 34], [13, 227, 5], [291, 196, 192, 323, 176, 48, 1, 49, 1, 49, 1, 49, 93, 93, 93], [22, 22, 189, 171], [13, 27, 9, 25, 4, 9, 3, 16, 6], [749, 53, 11, 18, 9, 45, 9], [116, 76, 94, 210, 476, 123, 393], [13, 16, 32], [5, 173, 14], [9, 547, 100, 32], [7, 635, 43, 71, 268, 283, 5], [8, 8, 4, 8, 3, 47], [788, 59, 97, 254, 258, 76, 42], [9, 80, 60], [81, 127, 396, 91], [7, 74, 11, 103, 168], [15, 161, 21, 161, 4, 15, 21, 3, 6, 16, 11, 16, 42, 23], [145, 11, 145, 544, 35], [105, 401, 109, 186, 76], [13, 77, 231, 4, 231, 3, 292], [13, 20, 334, 76, 11, 550], [7, 79, 244, 79, 86, 86], [26, 10, 11, 18, 2, 22, 22, 106, 2, 1, 147, 1, 19], [128, 6, 16, 190, 169, 11, 34], [731, 272, 206, 732, 221], [331, 112], [7, 63, 7, 456, 311, 63], [65, 34, 165, 32, 25], [7, 74, 216, 297], [314, 33, 115, 30, 4, 30, 3, 9, 25], [579, 381, 380, 121], [128, 555, 2, 4, 2, 3, 179, 346, 470, 4, 8, 3, 47, 45, 179], [16, 538, 43], [4, 3, 47, 85], [13, 39, 285], [330, 356, 63, 61, 63], [12, 25, 24], [4, 621, 211, 3, 6, 10, 23, 70], [34, 9, 80, 174], [12, 50, 104, 1, 49, 152, 58, 657, 58, 31], [13, 12, 55, 1, 87, 1, 87, 16, 32], [13, 7, 151, 7, 14, 451, 326], [6, 2, 4, 2, 3, 46, 26, 46, 6, 6, 11, 18, 394, 394], [4, 15, 21, 3, 6, 10, 23, 70], [37, 6], [7, 615, 39, 91], [207], [17, 3, 70], [7, 57, 559, 7, 166, 56], [132, 286, 125, 158, 467], [20, 43], [113, 282, 6, 47], [68, 163, 163, 163, 71, 163, 761, 59, 163], [9, 61, 522], [34, 71], [39, 25, 5], [20, 339, 20, 41, 218, 51], [113, 282, 47], [20, 402, 207, 207], [107, 43, 260, 829, 382, 43, 43, 61, 43], [20, 41, 25, 20, 41, 385, 550, 41], [57, 327, 19, 2, 329, 15, 5, 14, 19], [171, 417], [61], [13, 20, 41, 438, 67, 550], [317, 317], [7, 151, 7, 166, 56], [257, 37], [12, 50, 1, 1, 9], [28, 322, 5, 346, 470, 4, 8, 3, 47, 45, 5], [53, 11, 18, 72, 72], [65, 115, 9, 25], [5, 34, 149, 4, 149, 3, 352, 82, 25], [26, 117, 77, 11, 18, 98, 98], [524, 47], [796, 796, 31, 676], [28, 9, 25], [17, 32, 161, 32, 161, 70], [135, 35], [257, 8], [206, 206], [63, 310, 110, 1, 1, 5], [5, 212, 31], [7, 57, 82, 86, 16, 86, 778, 48, 85, 5], [13, 272, 367, 76, 272], [171, 110, 1, 1, 137, 147, 36, 1, 1, 162, 1, 104], [48, 39, 121, 8, 4, 8, 3, 47, 377, 39, 115], [34, 2, 4, 2, 3, 25], [66, 39, 121, 66, 33, 66, 8], [24, 5], [341, 11, 18, 2, 555, 2, 1, 62], [7, 53, 48, 230, 35, 88, 229, 35], [34], [7, 38, 29, 234], [77, 2, 190, 4, 2, 3, 37], [95, 95, 2, 19, 362], [46, 6, 138, 420], [7, 74, 39, 312, 45, 7, 103, 56, 139], [6, 47], [38, 15, 161, 4, 15, 3], [51, 43, 51, 78], [25, 6, 198, 6, 346, 470, 85, 4, 8, 3, 47, 45, 198, 6, 85], [28], [315, 102, 59, 77, 817, 1, 1], [46], [13, 16, 32, 25], [257, 287, 37, 137, 364, 1], [164, 4, 164, 3, 5], [113, 215, 113, 215, 416], [29, 4, 29, 621, 3, 153, 231, 17, 621, 84], [13, 166, 2, 3, 5, 4, 2, 3, 14], [7, 74, 216, 368, 33, 190, 827], [5, 39, 312, 5, 245, 100, 14], [34, 9], [195, 195, 71, 1, 1], [6, 46, 188], [180, 20, 41], [6, 99, 255], [31, 5, 14, 4, 5, 14, 3, 46, 5, 14], [1, 1, 40, 1, 1], [184, 91, 11], [2, 4, 2, 3, 6, 10, 8, 4, 8, 3, 47, 45, 6, 10], [20, 339, 340, 1], [7, 74, 405, 30, 553, 19, 19, 19, 33], [322, 121], [73, 12, 50, 1, 1, 73], [19, 531, 4, 2, 3, 4, 19, 3, 2, 163], [13, 7, 151, 7, 5, 68], [189, 22, 22, 189, 110, 1, 1, 162, 1, 131, 1, 9], [762, 289, 442, 5, 14, 31], [330, 1, 1, 9, 1, 1, 9, 1, 1, 467, 9], [28, 18, 2, 2, 3, 784, 165, 73, 1, 1, 1, 1], [17, 833, 3, 54, 165, 96, 82, 70], [7, 7, 5, 7, 14], [65, 6, 46, 6], [12, 172, 64, 13, 64, 24, 12], [39, 77, 5, 5, 56], [34, 789, 9], [38, 1, 1, 38], [20, 41, 198, 207, 20, 41, 445, 101, 198, 207], [4, 15, 21, 3, 6, 10, 23, 70], [24, 59, 5], [331, 71], [7, 74, 390, 390, 133, 29, 88, 33], [6], [12, 50, 37, 37], [542, 5, 209, 112], [297, 161, 592, 161], [27, 15, 21, 27, 15, 4, 15, 21, 3, 6, 27, 23, 17, 15, 3, 27], [19, 4, 2, 3, 6, 27, 23, 4, 19, 3, 2], [504, 69, 80, 60], [7, 191, 7, 29, 229, 35], [1, 58, 28, 30, 4, 30, 3, 597], [20, 339, 31, 676, 43], [8, 34, 9], [64, 365], [135], [5, 26, 5, 11, 18, 2, 2, 119, 35], [33, 4, 2, 3, 6, 10, 48, 17, 125, 2], [38, 8, 4, 9, 3, 6, 10, 169, 4, 10, 3, 790, 10, 4, 8, 3, 9], [72, 22, 22, 189, 72, 22, 224, 349], [12, 55, 1, 11, 52], [2, 4, 2, 3], [141], [111, 94, 325, 690], [22, 22], [801], [39, 2, 139], [5, 39, 121, 5], [13, 11, 4, 11, 3, 10, 17, 10, 11], [341, 2, 4, 2, 3, 285, 6, 17, 2], [16, 538, 44], [13, 28, 24, 8, 4, 8, 3], [43, 297, 42], [7, 42, 7, 42, 24, 112], [78, 389, 78, 43, 16], [27, 84, 34, 84, 4, 84, 3, 75], [13, 314, 201, 4, 1, 49, 89, 1, 49, 1, 1, 43, 201], [57, 63, 47], [22, 22, 171, 22, 22, 189], [11, 5, 11, 14, 4, 5, 14, 3, 46, 10, 11, 10, 11], [12, 50, 1, 338, 36, 1, 140, 9], [44, 74, 44, 63, 65, 18, 2, 2, 183], [28, 68, 25], [26, 10, 11, 10, 76], [28, 618, 4, 618, 3, 46, 75, 75], [5, 72, 14, 72], [648, 146, 649, 650, 496, 651, 35], [27, 2, 25, 4, 2, 3, 9, 25], [447, 20, 242, 125, 437, 197, 221], [13, 61], [6, 643, 6, 47], [27, 43, 395, 43, 4, 43, 3, 125, 6, 138], [4, 73, 3, 132, 352, 70], [34], [37, 346, 470, 85, 4, 8, 3, 47, 45, 37, 85], [37, 1, 1], [9, 4, 9, 3, 125, 6, 17, 268, 3, 9], [63, 310], [27, 10, 190, 85, 4, 10, 85, 3, 46, 37, 37], [422, 12, 124, 1, 94, 78, 123, 1, 36, 1, 49, 12], [44, 136, 18, 2, 26, 2], [13, 19, 4, 2, 3, 6, 27, 23, 4, 19, 3, 2], [7, 74, 262, 262, 168, 74, 216, 128], [6], [132, 41, 192], [26, 608, 11, 18, 10, 10, 76, 11, 76, 119, 35], [178, 289, 683, 683, 18, 2, 19, 2, 19], [7, 74, 57, 171, 104], [459, 221], [316, 560, 316, 88], [12, 25, 735, 736], [8, 4, 8, 66, 3, 46, 5, 14, 17, 8, 66], [96, 96, 71, 96], [31, 14, 8, 4, 8, 3, 29], [135, 387, 64], [44, 80, 174], [7, 57, 82, 86, 101, 96, 237, 807, 43], [4, 2, 3, 47, 168], [207, 41, 20, 41, 218, 20, 41, 445, 51], [39, 39, 2, 98, 19, 98, 83, 98, 362, 72], [26, 292, 11, 18, 2, 5, 2, 17, 2, 801, 9, 247, 14, 2], [9, 9, 59, 1, 58, 1, 49, 528, 1, 58], [132, 352, 82], [16, 32, 69, 32, 235], [2, 65, 34, 2, 25], [30, 4, 30, 3, 5, 17, 30], [639, 164], [38, 5, 14, 537, 5, 14, 31], [129, 146, 252, 1, 1, 5], [1, 1, 1, 1, 1, 62, 1, 1, 1, 80], [28, 322, 9], [107, 112], [9, 1, 1], [357, 343, 278, 399], [104, 33, 56, 33, 56], [39, 734, 8, 66, 5, 14], [16, 32, 112, 1, 58], [13, 275, 89, 134, 1, 87], [19, 24, 34, 2], [13, 9, 25], [63], [539, 429, 51, 96, 51, 96], [7, 5, 24, 426], [7, 141], [5, 39, 121, 26, 5, 11, 173], [206, 518, 204], [20, 108, 816], [104, 213, 4, 104, 3, 12], [38, 29, 63, 553, 211], [63], [128, 341, 16, 42, 128, 42, 341, 42], [123], [7, 63, 24, 7, 63, 369, 311, 110, 1, 295, 147, 1, 19], [39, 121, 2, 44, 362, 2, 168, 1, 1, 72], [5, 5, 33], [72, 26, 6, 5, 23, 11, 18, 2, 2], [493], [34, 9, 25], [12, 124, 1, 1, 1, 1, 5, 516, 1, 1, 5, 516], [6, 2, 169, 29, 31], [108, 236, 154], [5, 79, 86, 64, 86, 771, 33], [34, 2, 4, 2, 3, 37], [141], [115, 8, 4, 8, 3, 75], [20, 41, 385, 51], [99, 255, 6, 47, 436], [40, 180, 52, 79, 58], [5, 5, 1, 58, 15, 5, 85], [788, 59, 97, 254, 258, 42, 52], [499, 414, 183, 234], [], [16, 93, 3, 303], [111, 188, 26, 27, 23], [26, 6, 269, 23, 11, 18, 5, 65, 34, 5, 25], [39, 39, 7, 33, 56], [257, 287, 40, 257, 40], [150, 6, 429, 281, 6]]\n",
            "Size of vocabulary in X = 848\n",
            "% of rare words in vocabulary: 41.911764705882355\n",
            "Size of vocabulary in Y = 791\n",
            "This is X_vocaboulary:-\n",
            "[[157  38 574 ...   0   0   0]\n",
            " [109  76  76 ...   0   0   0]\n",
            " [ 33  45  19 ...   0   0   0]\n",
            " ...\n",
            " [129 109 475 ...   0   0   0]\n",
            " [113 225   0 ...   0   0   0]\n",
            " [ 24  69  60 ...   0   0   0]]\n",
            "This is Y_vocaboulary:-\n",
            "[[  1   7  20 ...   0   0   0]\n",
            " [  1 254  32 ...   0   0   0]\n",
            " [  1 330   5 ...   0   0   0]\n",
            " ...\n",
            " [  1 263  37 ...   0   0   0]\n",
            " [  1   4   7 ...   0   0   0]\n",
            " [  1   7   6 ...   0   0   0]]\n",
            "[157  38 574 575   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0]\n",
            "[  1   7  20  15  77   3 569 570  13  65   2   0   0   0   0   0   0   0\n",
            "   0   0]\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 25)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 25, 200)      169600      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 25, 300),    601200      ['embedding[0][0]']              \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 25, 300),    721200      ['lstm[0][0]']                   \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 200)    158200      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 25, 300),    721200      ['lstm_1[0][0]']                 \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 300),  601200      ['embedding_1[0][0]',            \n",
            "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " time_distributed (TimeDistribu  (None, None, 791)   238091      ['lstm_3[0][0]']                 \n",
            " ted)                                                                                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,210,691\n",
            "Trainable params: 3,210,691\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "########### 1 (80, 50)============================================================\n",
            "Epoch 1/50\n",
            "15/15 [==============================] - 20s 509ms/step - loss: 3.4142 - accuracy: 0.4949 - val_loss: 2.4434 - val_accuracy: 0.5530\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 7s 443ms/step - loss: 2.5046 - accuracy: 0.5532 - val_loss: 2.3185 - val_accuracy: 0.5669\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 6s 432ms/step - loss: 2.3603 - accuracy: 0.5710 - val_loss: 2.1848 - val_accuracy: 0.6118\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 7s 436ms/step - loss: 2.2422 - accuracy: 0.5922 - val_loss: 2.1184 - val_accuracy: 0.6168\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 7s 442ms/step - loss: 2.1729 - accuracy: 0.6057 - val_loss: 2.0486 - val_accuracy: 0.6179\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 8s 539ms/step - loss: 2.1108 - accuracy: 0.6130 - val_loss: 1.9858 - val_accuracy: 0.6265\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 7s 437ms/step - loss: 2.0364 - accuracy: 0.6312 - val_loss: 1.9604 - val_accuracy: 0.6295\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 7s 442ms/step - loss: 1.9642 - accuracy: 0.6467 - val_loss: 1.8534 - val_accuracy: 0.6608\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 6s 433ms/step - loss: 1.8954 - accuracy: 0.6546 - val_loss: 1.8239 - val_accuracy: 0.6608\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 8s 547ms/step - loss: 1.8482 - accuracy: 0.6600 - val_loss: 1.7809 - val_accuracy: 0.6610\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 7s 442ms/step - loss: 1.7940 - accuracy: 0.6643 - val_loss: 1.7369 - val_accuracy: 0.6701\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 7s 445ms/step - loss: 1.7451 - accuracy: 0.6689 - val_loss: 1.7031 - val_accuracy: 0.6727\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 7s 437ms/step - loss: 1.7002 - accuracy: 0.6732 - val_loss: 1.6692 - val_accuracy: 0.6792\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 7s 450ms/step - loss: 1.6566 - accuracy: 0.6775 - val_loss: 1.6516 - val_accuracy: 0.6821\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 7s 436ms/step - loss: 1.6239 - accuracy: 0.6800 - val_loss: 1.6262 - val_accuracy: 0.6831\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 7s 439ms/step - loss: 1.5857 - accuracy: 0.6846 - val_loss: 1.5975 - val_accuracy: 0.6879\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 7s 433ms/step - loss: 1.5541 - accuracy: 0.6885 - val_loss: 1.5821 - val_accuracy: 0.6888\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 6s 433ms/step - loss: 1.5218 - accuracy: 0.6937 - val_loss: 1.5661 - val_accuracy: 0.6935\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 7s 439ms/step - loss: 1.4894 - accuracy: 0.6972 - val_loss: 1.5444 - val_accuracy: 0.6972\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 7s 444ms/step - loss: 1.4605 - accuracy: 0.6994 - val_loss: 1.5353 - val_accuracy: 0.6995\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 7s 449ms/step - loss: 1.4350 - accuracy: 0.7019 - val_loss: 1.5191 - val_accuracy: 0.7000\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 7s 442ms/step - loss: 1.4072 - accuracy: 0.7060 - val_loss: 1.5013 - val_accuracy: 0.7032\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 7s 442ms/step - loss: 1.3762 - accuracy: 0.7094 - val_loss: 1.4979 - val_accuracy: 0.7022\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 7s 442ms/step - loss: 1.3493 - accuracy: 0.7139 - val_loss: 1.4803 - val_accuracy: 0.7066\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 7s 448ms/step - loss: 1.3247 - accuracy: 0.7165 - val_loss: 1.4667 - val_accuracy: 0.7094\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 7s 446ms/step - loss: 1.3012 - accuracy: 0.7188 - val_loss: 1.4731 - val_accuracy: 0.7079\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 7s 443ms/step - loss: 1.2761 - accuracy: 0.7238 - val_loss: 1.4538 - val_accuracy: 0.7117\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 7s 443ms/step - loss: 1.2530 - accuracy: 0.7265 - val_loss: 1.4546 - val_accuracy: 0.7091\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 7s 450ms/step - loss: 1.2305 - accuracy: 0.7290 - val_loss: 1.4364 - val_accuracy: 0.7112\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 7s 446ms/step - loss: 1.2060 - accuracy: 0.7324 - val_loss: 1.4272 - val_accuracy: 0.7128\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 7s 442ms/step - loss: 1.1897 - accuracy: 0.7348 - val_loss: 1.4216 - val_accuracy: 0.7162\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 7s 449ms/step - loss: 1.1644 - accuracy: 0.7384 - val_loss: 1.4239 - val_accuracy: 0.7180\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 7s 445ms/step - loss: 1.1501 - accuracy: 0.7393 - val_loss: 1.4026 - val_accuracy: 0.7185\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 7s 438ms/step - loss: 1.1269 - accuracy: 0.7439 - val_loss: 1.4106 - val_accuracy: 0.7173\n",
            "Epoch 35/50\n",
            "15/15 [==============================] - 7s 447ms/step - loss: 1.1060 - accuracy: 0.7480 - val_loss: 1.3994 - val_accuracy: 0.7177\n",
            "Epoch 36/50\n",
            "15/15 [==============================] - 7s 450ms/step - loss: 1.0890 - accuracy: 0.7501 - val_loss: 1.3974 - val_accuracy: 0.7198\n",
            "Epoch 37/50\n",
            "15/15 [==============================] - 7s 443ms/step - loss: 1.0703 - accuracy: 0.7540 - val_loss: 1.3940 - val_accuracy: 0.7238\n",
            "Epoch 38/50\n",
            "15/15 [==============================] - 7s 454ms/step - loss: 1.0537 - accuracy: 0.7552 - val_loss: 1.3846 - val_accuracy: 0.7221\n",
            "Epoch 39/50\n",
            "15/15 [==============================] - 7s 456ms/step - loss: 1.0355 - accuracy: 0.7595 - val_loss: 1.3820 - val_accuracy: 0.7191\n",
            "Epoch 40/50\n",
            "15/15 [==============================] - 7s 460ms/step - loss: 1.0199 - accuracy: 0.7619 - val_loss: 1.3812 - val_accuracy: 0.7245\n",
            "Epoch 41/50\n",
            "15/15 [==============================] - 7s 457ms/step - loss: 0.9996 - accuracy: 0.7662 - val_loss: 1.3896 - val_accuracy: 0.7238\n",
            "Epoch 42/50\n",
            "15/15 [==============================] - 7s 448ms/step - loss: 0.9866 - accuracy: 0.7676 - val_loss: 1.3722 - val_accuracy: 0.7286\n",
            "Epoch 43/50\n",
            "15/15 [==============================] - 7s 452ms/step - loss: 0.9667 - accuracy: 0.7723 - val_loss: 1.3773 - val_accuracy: 0.7277\n",
            "Epoch 44/50\n",
            "15/15 [==============================] - 7s 451ms/step - loss: 0.9522 - accuracy: 0.7745 - val_loss: 1.3742 - val_accuracy: 0.7274\n",
            "Epoch 44: early stopping\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Code: 'ax' 'set' 'xlabel' 'temperature' \n",
            "Original summary: 'how' 'do' 'i' 'print' 'a' 'celsius' 'symbol' 'with' 'matplotlib' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'current' 'name'\n",
            "BLEU Score: 1.375558813346999e-231\n",
            "\n",
            "\n",
            "Code: 'request' 'get' 'get' 'username' \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original summary: 'multi' 'value' 'dict' 'key' 'error' 'in' 'django' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'current' 'command'\n",
            "BLEU Score: 1.3509119634545632e-231\n",
            "\n",
            "\n",
            "Code: 'sum' 'len' 'y' 'for' 'y' 'in' 'x' 'if' 'len' 'y' \n",
            "Original summary: 'statement' 'in' 'python' 'to' 'return' 'the' 'sum' 'of' 'certain' 'lists' 'in' 'a' 'list' 'of' 'lists' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2769777614215993e-231\n",
            "\n",
            "\n",
            "Code: 'sorted' 'trial' 'list' 'key' 'lambda' 'x' 'trial' 'dict' 'x' \n",
            "Original summary: 'sort' 'a' 'list' 'based' 'on' 'dictionary' 'values' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.253537017077218e-231\n",
            "\n",
            "\n",
            "Code: 'pickle' 'itemlist' 'outfile' \n",
            "Original summary: 'writing' 'a' 'list' 'to' 'a' 'file' 'with' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'element' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2655806872459487e-231\n",
            "\n",
            "\n",
            "Code: 'trace' 'df' 'mean' \n",
            "Original summary: 'match' 'in' 'pandas' 'dataframe' \n",
            "Predicted summary:  'pandas' 'data' 'frame' 'to' 'list' 'of' 'columns'\n",
            "BLEU Score: 1.318705411715495e-231\n",
            "\n",
            "\n",
            "Code: 'sum' 'x x' 'for' 'x' 'in' 'range' 'len' \n",
            "Original summary: 'find' 'the' 'sum' 'of' 'subsets' 'of' 'a' 'list' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'tuples' 'in' 'python'\n",
            "BLEU Score: 1.3048555118335446e-231\n",
            "\n",
            "\n",
            "Code: 'src' 'dst' \n",
            "Original summary: 'copy' 'a' 'file' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'current' 'command'\n",
            "BLEU Score: 1.1134712866512498e-231\n",
            "\n",
            "\n",
            "Code: 't' 'strip' 'for' 's' 'in' 'string' 'split' 'for' 't' 'in' 's' 'split' \n",
            "Original summary: 'python' 'split' 'string' 'by' 'list' 'of' 'separators' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'name' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2395288183339461e-231\n",
            "\n",
            "\n",
            "Code: 'x' 'none' 'for' 'in' 'range' 'for' 'in' 'range' \n",
            "Original summary: 'creating' 'a' 'd' 'matrix' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'largest' 'item' 'from' 'a' 'list' 'in' 'python'\n",
            "BLEU Score: 1.2097822504111573e-231\n",
            "\n",
            "\n",
            "Code: 'ax' 'set' 'title' ' s' ' ' ' ' s' ' ' \n",
            "Original summary: 'how' 'can' 'i' 'format' 'a' 'float' 'using' 'matplotlib' 's' 'la' 'te' 'x' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'current' 'command' 'in' 'a' 'python' 'script'\n",
            "BLEU Score: 1.278927847674264e-231\n",
            "\n",
            "\n",
            "Code: 'df' 'df' 'a' 'isin' \n",
            "Original summary: 'use' 'a' 'list' 'of' 'values' 'to' 'select' 'rows' 'from' 'a' 'pandas' 'dataframe' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'data' 'frame' 'in' 'pandas' 'dataframe'\n",
            "BLEU Score: 1.2726119615895568e-231\n",
            "\n",
            "\n",
            "Code: 're' 'sub' ' ' 'as' 'a' \n",
            "Original summary: 'removing' 'all' 'non numeric' 'characters' 'from' 'string' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'size' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2395288183339461e-231\n",
            "\n",
            "\n",
            "Code: 'join' \n",
            "Original summary: 'how' 'do' 'i' 'convert' 'an' 'array' 'to' 'string' 'using' 'the' 'jinja' 'template' 'engine' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.297880089357498e-231\n",
            "\n",
            "\n",
            "Code: 'x' 'for' 'x' 'in' 'a' 'if' 'x' 'not' 'in' \n",
            "Original summary: 'remove' 'all' 'values' 'within' 'one' 'list' 'from' 'another' 'list' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2991943999925157e-231\n",
            "\n",
            "\n",
            "Code: 're' 'search' ' bis b' 'string' 'start' \n",
            "Original summary: 'python' 'locating' 'the' 'position' 'of' 'a' 'regex' 'match' 'in' 'a' 'string' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'name' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2596903697536756e-231\n",
            "\n",
            "\n",
            "Code: 'join' 'c' 'for' 'c' 'in' 'if' 'c' 'isdigit' \n",
            "Original summary: 'removing' 'letters' 'from' 'a' 'list' 'of' 'numbers' 'and' 'letters' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.1896457329133973e-231\n",
            "\n",
            "\n",
            "Code: 'print' 'x ' 'x' ' ' 'value' \n",
            "Original summary: 'how' 'can' 'i' 'get' 'python' 'to' 'use' 'upper' 'case' 'letters' 'to' 'print' 'hex' 'values' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.297880089357498e-231\n",
            "\n",
            "\n",
            "Code: 'np' 'np' 'all' 'arr' 'axis' \n",
            "Original summary: 'how' 'to' 'find' 'row' 'of' 'd' 'array' 'in' 'd' 'numpy' 'array' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'data' 'frame' 'in' 'pandas'\n",
            "BLEU Score: 1.318705411715495e-231\n",
            "\n",
            "\n",
            "Code: 'img' 'image' 'open' 'picture' 'jpg' 'img' 'show' \n",
            "Original summary: 'open' 'images' \n",
            "Predicted summary:  'using' 'beautiful' 'soup' 'to' 'post' 'html'\n",
            "BLEU Score: 1.2739929858261433e-231\n",
            "\n",
            "\n",
            "Code: 'pd' 'data' 'frame' 'd' \n",
            "Original summary: 'convert' 'list' 'of' 'dictionaries' 'to' 'dataframe' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'data' 'frame' 'in' 'python' 'pandas' 'by' 'two' 'or' 'more' 'columns'\n",
            "BLEU Score: 1.1545437674642806e-231\n",
            "\n",
            "\n",
            "Code: 'with' 'open' 'foo' 'a' 'as' 'f' 'f' 'write' \n",
            "Original summary: 'append' 'to' 'a' 'file' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'current' 'name'\n",
            "BLEU Score: 1.1783324442592282e-231\n",
            "\n",
            "\n",
            "Code: 'list' 'map' 'list' 'set' 'map' 'lambda' 'i' 'tuple' 'i' 'testdata' \n",
            "Original summary: 'python' 'uniqueness' 'for' 'list' 'of' 'lists' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.253537017077218e-231\n",
            "\n",
            "\n",
            "Code: 's' 'split' 's' \n",
            "Original summary: 'string' 'splitting' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'name' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.1722739790570059e-231\n",
            "\n",
            "\n",
            "Code: 'json' 'dumps' 'your' 'data' 'ascii' 'false' \n",
            "Original summary: 'convert' 'python' 'dictionary' 'to' 'json' 'array' \n",
            "Predicted summary:  'python' 'how' 'to' 'get' 'rid' 'of' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2429576998234947e-231\n",
            "\n",
            "\n",
            "Code: 'plt' \n",
            "Original summary: 'how' 'do' 'i' 'tell' 'matplotlib' 'that' 'i' 'am' 'with' 'a' 'plot' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'name' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.1470490375538403e-231\n",
            "\n",
            "\n",
            "Code: 'random' 'choice' \n",
            "Original summary: 'how' 'do' 'i' 'select' 'a' 'random' 'element' 'from' 'an' 'array' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'largest' 'item' 'from' 'a' 'list' 'in' 'python'\n",
            "BLEU Score: 1.269952360927919e-231\n",
            "\n",
            "\n",
            "Code: 'raise' 'exception' 'this' 'is' 'the' 'exception' 'you' 'to' \n",
            "Original summary: 'manually' 'raising' 'throwing' 'an' 'exception' \n",
            "Predicted summary:  'how' 'to' 'get' 'a' 'file' 'with' 'the' 'application'\n",
            "BLEU Score: 1.294045837453707e-231\n",
            "\n",
            "\n",
            "Code: 'filter' 'lambda' 'x' 'len' 'x' \n",
            "Original summary: 'filtering' 'df' 'in' 'pandas' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2287022022568943e-231\n",
            "\n",
            "\n",
            "Code: 'matrix' 'for' 'i' 'in' 'range' \n",
            "Original summary: 'how' 'to' 'define' dimensional' 'array' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2991943999925157e-231\n",
            "\n",
            "\n",
            "Code: 'df' 'to' 'dict' 'index' \n",
            "Original summary: 'pandas' 'data' 'frame' 'to' 'list' 'of' 'dictionaries' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'data' 'frame' 'in' 'python' 'pandas' 'by' 'two' 'or' 'more' 'columns'\n",
            "BLEU Score: 1.1733229948528878e-231\n",
            "\n",
            "\n",
            "Code: 'url' 're' 'sub' ' ' 'com' 'url' \n",
            "Original summary: 'remove' 'a' 'substring' 'from' 'the' 'end' 'of' 'a' 'string' \n",
            "Predicted summary:  'how' 'to' 'use' 'a' 'python' 'script' 'which' 'can' 'logoff' 'shutdown' 'and' 'restart' 'a' 'computer'\n",
            "BLEU Score: 1.1409851298103347e-231\n",
            "\n",
            "\n",
            "Code: 'int' 'round' 'x' \n",
            "Original summary: 'round' 'number' 'to' 'nearest' 'integer' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.1640469867513693e-231\n",
            "\n",
            "\n",
            "Code: 'a' 'for' 'i' 'in' 'range' \n",
            "Original summary: 'python' 'how' 'to' 'append' 'new' 'elements' 'in' 'a' 'list' 'of' 'list' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2769777614215993e-231\n",
            "\n",
            "\n",
            "Code: 'os' 'environ' 'home' \n",
            "Original summary: 'access' 'environment' 'variables' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'current' 'command'\n",
            "BLEU Score: 1.2651476398987394e-231\n",
            "\n",
            "\n",
            "Code: 'entry' 'objects' 'filter' 'name' 'name' 'title' 'title' 'exists' \n",
            "Original summary: 'how' 'to' 'check' 'if' 'something' 'exists' 'in' 'a' 'database' 'using' 'django' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2136911292873877e-231\n",
            "\n",
            "\n",
            "Code: 'foo' 'append' \n",
            "Original summary: 'add' 'to' 'integers' 'in' 'a' 'list' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.1640469867513693e-231\n",
            "\n",
            "\n",
            "Code: 'url' 'rsplit' \n",
            "Original summary: 'how' 'to' 'get' 'everything' 'after' 'last' 'slash' 'in' 'a' 'url' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'size' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2777127129554467e-231\n",
            "\n",
            "\n",
            "Code: 'alist' \n",
            "Original summary: 'empty' 'a' 'list' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'largest' 'item' 'from' 'a' 'list' 'in' 'python'\n",
            "BLEU Score: 1.1389990733180202e-231\n",
            "\n",
            "\n",
            "Code: 'list' 'abcd' \n",
            "Original summary: 'what' 's' 'a' 'way' 'to' 'through' 'a' 'set' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'largest' 'item' 'from' 'a' 'list' 'in' 'python'\n",
            "BLEU Score: 1.1640469867513693e-231\n",
            "\n",
            "\n",
            "Code: 'df' 'set' 'index' 'year' 'month' 'item' 'unstack' 'level' \n",
            "Original summary: 'pandas' 'how' 'to' 'run' 'a' 'pivot' 'with' 'a' 'multi index' \n",
            "Predicted summary:  'pandas' 'data' 'frame' 'how' 'to' 'filter' 'by' 'column'\n",
            "BLEU Score: 1.3404899290088416e-231\n",
            "\n",
            "\n",
            "Code: ' 'abc' 'encode' 'hex' \n",
            "Original summary: 'print' 'string' 'as' 'hex' 'literal' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'name' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2183324802375697e-231\n",
            "\n",
            "\n",
            "Code: 'pd' 'concat' 'df' 'dates' 'df' 'dates' 'axis' \n",
            "Original summary: 'how' 'to' 'merge' 'two' 'data' 'frames' 'into' 'single' 'matching' 'the' 'column' 'values' \n",
            "Predicted summary:  'pandas' 'pandas' 'how' 'to' 'get' 'the' 'row' 'names' 'from' 'a' 'column'\n",
            "BLEU Score: 1.2751495852793276e-231\n",
            "\n",
            "\n",
            "Code: 'dict' 'v' 'k' 'for' 'k' 'v' 'in' 'my' 'dict' 'items' \n",
            "Original summary: 'keys' 'and' 'values' 'in' 'a' 'dictionary' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.253537017077218e-231\n",
            "\n",
            "\n",
            "Code: 's' 'decode' 'unicode' 'escape' \n",
            "Original summary: 'how' 'to' 'decode' 'unicode' 'raw' 'literals' 'to' 'readable' 'string' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'name' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.195968253499657e-231\n",
            "\n",
            "\n",
            "Code: 'root' \n",
            "Original summary: 'how' 'to' 'make' 'a' 'window' 'jump' 'to' 'the' 'front' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'size' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2328102575634849e-231\n",
            "\n",
            "\n",
            "Code: 'for' 'dirname' 'dirnames' 'filenames' 'in' 'os' 'walk' 'for' 'subdirname' 'in' 'dirnames' 'print' 'os' 'path' 'join' 'dirname' 'subdirname' 'for' 'filename' 'in' 'filenames' 'pass' \n",
            "Original summary: 'directory' 'listing' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.1896457329133973e-231\n",
            "\n",
            "\n",
            "Code: 'raise' 'exception' 'i' 'know' 'python' \n",
            "Original summary: 'manually' 'raising' 'throwing' 'an' 'exception' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'current' 'command'\n",
            "BLEU Score: 1.3761735883890566e-231\n",
            "\n",
            "\n",
            "Code: 'open' 'file' 'gz' 'encoding' 'utf' \n",
            "Original summary: 'reading' 'utf' 'characters' 'from' 'a' 'file' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'current' 'command' 'in' 'python'\n",
            "BLEU Score: 1.384292958842266e-231\n",
            "\n",
            "\n",
            "Code: 'datetime' 'datetime' 'strptime' 'sep' ' d b ' 'y' \n",
            "Original summary: 'how' 'to' 'create' 'datetime' 'object' 'from' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'name' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2395288183339461e-231\n",
            "\n",
            "\n",
            "Code: 'os' 'chdir' 'c ' ' chapter' \n",
            "Original summary: 'change' 'current' 'working' 'directory' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'a' 'file' 'in' 'python'\n",
            "BLEU Score: 1.3588822913065803e-231\n",
            "\n",
            "\n",
            "Code: 'hex' 'd' 'split' 'x' \n",
            "Original summary: 'python' 'convert' 'decimal' 'to' 'hex' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2363867681772576e-231\n",
            "\n",
            "\n",
            "Code: 'os' 'listdir' 'path' \n",
            "Original summary: 'how' 'can' 'i' 'list' 'the' 'contents' 'of' 'a' 'directory' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'size' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2777127129554467e-231\n",
            "\n",
            "\n",
            "Code: 'os' 'listdir' \n",
            "Original summary: 'list' 'all' 'files' 'of' 'a' 'directory' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'size' 'of' 'a' 'string'\n",
            "BLEU Score: 1.184298875206812e-231\n",
            "\n",
            "\n",
            "Code: 'x' 'x' 'format' \n",
            "Original summary: 'python' 'how' 'to' 'convert' 'int' 'to' 'string' 'a' 'bit' 'hex' 'number' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.297880089357498e-231\n",
            "\n",
            "\n",
            "Code: 're' 'split' ' s' ' s' ' s' ' s' 'a' 'b' \n",
            "Original summary: 'python' 'split' 'string' 'by' 'list' 'of' 'separators' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'size' 'of' 'a' 'string'\n",
            "BLEU Score: 1.2645824632376784e-231\n",
            "\n",
            "\n",
            "Code: 'x' 'y' 'for' 'x' 'y' 'in' 'zip' 'first' 'second' \n",
            "Original summary: 'add' 'sum' 'of' 'values' 'of' 'two' 'lists' 'into' 'new' 'list' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2022630406526483e-231\n",
            "\n",
            "\n",
            "Code: 'os' 'chdir' \n",
            "Original summary: 'how' 'to' 'move' 'to' 'one' 'folder' 'back' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'size' 'of' 'a' 'file' 'in' 'python'\n",
            "BLEU Score: 1.2662006142931263e-231\n",
            "\n",
            "\n",
            "Code: 'replace' 'and' \n",
            "Original summary: 'replace' 'part' 'of' 'a' 'string' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'size' 'of' 'a' 'string'\n",
            "BLEU Score: 1.2645824632376784e-231\n",
            "\n",
            "\n",
            "Code: 'try' 'del' 'mydict' 'key' 'except' 'key' 'error' 'pass' 'try' 'del' 'mydict' 'key' 'except' 'key' 'error' 'pass' \n",
            "Original summary: 'delete' 'a' 'dictionary' 'item' 'if' 'the' 'key' 'exists' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'largest' 'item' 'from' 'a' 'list' 'in' 'python'\n",
            "BLEU Score: 1.2308298330855597e-231\n",
            "\n",
            "\n",
            "Code: 'your' 'list' 'sort' 'key' 'operator' 'attrgetter' 'anniversary' 'score' \n",
            "Original summary: 'sorting' 'a' 'list' 'with' 'objects' 'of' 'a' 'class' 'as' 'its' 'items' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2769777614215993e-231\n",
            "\n",
            "\n",
            "Code: 'print' ' ' 'f' ' ' 'val' 'for' 'val' 'in' 'l' \n",
            "Original summary: 'control' 'a' 'print' 'format' 'when' 'printing' 'a' 'list' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'name' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.278927847674264e-231\n",
            "\n",
            "\n",
            "Code: 'j' 'for' 'i' 'in' 'x' 'for' 'j' 'in' 'i' \n",
            "Original summary: 'join' 'list' 'of' 'lists' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2287022022568943e-231\n",
            "\n",
            "\n",
            "Code: 'isinstance' 'x' 'int' \n",
            "Original summary: 'checking' 'whether' 'a' 'variable' 'is' 'an' 'integer' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2363867681772576e-231\n",
            "\n",
            "\n",
            "Code: 'numpy' 'where' 'x' \n",
            "Original summary: 'find' 'indices' 'of' 'elements' 'equal' 'to' 'zero' 'from' 'numpy' 'array' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2136911292873877e-231\n",
            "\n",
            "\n",
            "Code: 'sorted' 'sorted' 's' 'key' 'str' 'upper' \n",
            "Original summary: 'sort' 'a' 'string' 'in' 'lexicographic' 'order' 'python' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2769777614215993e-231\n",
            "\n",
            "\n",
            "Code: 'join' 'set' 'foo' \n",
            "Original summary: 'removing' 'duplicate' 'characters' 'from' 'a' 'string' \n",
            "Predicted summary:  'how' 'to' 'convert' 'a' 'string' 'to' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2578972116573994e-231\n",
            "\n",
            "\n",
            "Code: 'map' 'list' 'zip' \n",
            "Original summary: 'convert' 'list' 'of' 'tuples' 'to' 'multiple' 'lists' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.253537017077218e-231\n",
            "\n",
            "\n",
            "Code: 'ax' 'xaxis' 'set' 'ticks' 'position' 'top' \n",
            "Original summary: 'how' 'to' 'plot' 'with' 'x axis' 'at' 'the' 'top' 'of' 'the' 'figure' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'name' 'of' 'a' 'string' 'in' 'python'\n",
            "BLEU Score: 1.2183324802375697e-231\n",
            "\n",
            "\n",
            "Code: 'a' 'update' \n",
            "Original summary: 'python' 'append' 'values' 'to' 'a' 'set' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'first' 'element' 'of' 'a' 'list' 'in' 'python'\n",
            "BLEU Score: 1.168124271540546e-231\n",
            "\n",
            "\n",
            "Code: 'requests' 'get' 'url' 'headers' 'referer' 'my' 'referer' \n",
            "Original summary: 'changing' 'the' 'url' 'in' 'python' 'requests' \n",
            "Predicted summary:  'python' 'beautiful' 'soup' 'get' 'post' 'function'\n",
            "BLEU Score: 1.3568676344828118e-231\n",
            "\n",
            "\n",
            "Code: 'len' 'set' 'a' 'len' 'a' \n",
            "Original summary: 'how' 'to' 'check' 'whether' 'elements' 'in' 'the' 'list' 'only' 'once' 'in' 'python' \n",
            "Predicted summary:  'how' 'to' 'get' 'the' 'largest' 'item' 'from' 'a' 'list' 'in' 'python'\n",
            "BLEU Score: 1.2308298330855597e-231\n",
            "\n",
            "\n",
            "Code: 'v' 'k' 'for' 'k' 'v' 'in' 'd' 'items' \n",
            "Original summary: 'how' 'can' 'i' 'convert' 'a' 'python' 'dictionary' 'to' 'a' 'list' 'of' 'tuples' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2991943999925157e-231\n",
            "\n",
            "\n",
            "Code: 'dict' 'zip' 'my' 'list' 'map' 'my' 'dictionary' 'get' 'my' 'list' \n",
            "Original summary: 'is' 'it' 'possible' 'to' 'take' 'an' 'ordered' 'slice' 'of' 'a' 'dictionary' 'in' 'python' 'based' 'on' 'a' 'list' 'of' 'keys' \n",
            "Predicted summary:  'how' 'to' 'sort' 'a' 'list' 'of' 'strings' 'in' 'python'\n",
            "BLEU Score: 1.2769777614215993e-231\n",
            "\n",
            "\n",
            "Code: 'df' 'set' 'index' 'id' 'to' 'dict' \n",
            "Original summary: 'python' 'pandas' 'dataframe' 'to' 'dictionary' \n",
            "Predicted summary:  'pandas' 'data' 'frame' 'to' 'list' 'of' 'dictionaries'\n",
            "BLEU Score: 1.310642152712693e-231\n",
            "\n",
            "\n",
            "avg_BLEU_Score:  5.118938755088504e-233\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "############################# Data Cleaning and Pre-Processing #########################################\n",
        "############  Function to remove non-alphabetic characters (Data Cleaning)\n",
        "def clean_text(column):\n",
        "    for row in column:\n",
        "        row = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1',  str(row))).split()\n",
        "        row = ' '.join(row)\n",
        "        row = re.sub(\"(\\\\t)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\\\r)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\\\n)\", \" \", str(row)).lower()\n",
        "        # Remove the characters - <>()|&\"',;?~*!\n",
        "        row = re.sub(r\"[<>()|&\\[\\]\\'\\\",.\\}`$\\{;@?~*!+=_\\//1234567890]\", \" \", str(row)).lower()\n",
        "        row = re.sub(r\"\\\\b(\\\\w+)(?:\\\\W+\\\\1\\\\b)+\", \"\", str(row)).lower()\n",
        "        # Replace INC nums to INC_NUM\n",
        "        row = re.sub(\"([iI][nN][cC]\\d+)\", \"INC_NUM\", str(row)).lower()\n",
        "        # Replace CM# and CHG# to CM_NUM\n",
        "        row = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", \"CM_NUM\", str(row)).lower()\n",
        "        # Remove punctuations at the end of a word\n",
        "        row = re.sub(\"(\\.\\s+)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\-\\s+)\", \" \", str(row)).lower()\n",
        "        row = re.sub(\"(\\:\\s+)\", \" \", str(row)).lower()\n",
        "        # Replace any url to only the domain name\n",
        "        try:\n",
        "            url = re.search(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", str(row))\n",
        "            repl_url = url.group(3)\n",
        "            row = re.sub(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", repl_url, str(row))\n",
        "        except:\n",
        "            pass\n",
        "        row = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1',  str(row))).split()\n",
        "        # Remove multiple spaces\n",
        "        row = re.sub(\"(\\s+)\", \" \", str(row)).lower()\n",
        "        # Remove the single character hanging between any two spaces\n",
        "        row = re.sub(\"(\\s+.\\s+)\", \" \", str(row)).lower()\n",
        "        yield row\n",
        "\n",
        "\n",
        "\n",
        "# Data Reading using Pandas DataFrame\n",
        "df_code = pd.read_csv('/content/drive/MyDrive/NLP/python_Sample_dataset.csv')\n",
        "df_code_p = df_code[[\"code\",\"docstring\"]]\n",
        "# Calling Clean text\n",
        "processed_code= clean_text(df_code_p['code'])\n",
        "processed_summary = clean_text(df_code_p['docstring'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import spacy\n",
        "from time import time\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) \n",
        "t = time()\n",
        "# Process text as batches and yield Doc objects in order\n",
        "code = [str(doc) for doc in nlp.pipe(processed_code, batch_size=50)]\n",
        "summary = [ str(doc)  for doc in nlp.pipe(processed_summary, batch_size=50)]\n",
        "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "# summary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(processed_summary, batch_size=50)]\n",
        "df_code_p['code'] = code\n",
        "df_code_p['docstring'] = summary\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_count = []\n",
        "summary_count = []\n",
        "# max_code_len = 0\n",
        "# max_summary_len = 0\n",
        "# avg_code_len =0\n",
        "# avg_summary_len = 0\n",
        "# print(df_code_p['code'])\n",
        "for sent in df_code_p['code']:\n",
        "    # print (sent.split())\n",
        "    text_count.append(len(sent.split()))\n",
        "    # max_code_len = max(max_code_len, len(sent.split()))\n",
        "    # avg_code_len += len(sent.split())\n",
        "    \n",
        "for sent in df_code_p['docstring']:\n",
        "    summary_count.append(len(sent.split()))\n",
        "    # max_summary_len = max(max_summary_len, len(sent.split()))\n",
        "    # avg_summary_len += len(sent.split())\n",
        "# n = len(text_count)\n",
        "# avg_code_len /= n\n",
        "# avg_summary_len /= n\n",
        "# print(max_code_len, max_summary_len)\n",
        "# print(avg_code_len, avg_summary_len)\n",
        "graph_df = pd.DataFrame() \n",
        "graph_df['text'] = text_count\n",
        "graph_df['summary'] = summary_count\n",
        "# print(graph_df)\n",
        "graph_df.hist(bins = 10)\n",
        "plt.show()\n",
        "\n",
        "max_code_len = 25\n",
        "max_summary_len = 20\n",
        "\n",
        "######    Select the Summaries and Text which fall below max length \n",
        "import numpy as np\n",
        "\n",
        "cleaned_code = np.array(df_code_p['code'])\n",
        "cleaned_summary= np.array(df_code_p['docstring'])\n",
        "short_text = []\n",
        "short_summary = []\n",
        "for i in range(len(cleaned_code)):\n",
        "    if len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_code[i].split()) <= max_code_len:\n",
        "        short_text.append(cleaned_code[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "post_code = pd.DataFrame({'code': short_text,'summary': short_summary})\n",
        "# print(post_code.head(100))\n",
        "\n",
        "post_code['summary'] = post_code['summary'].apply(lambda x: 'sostok ' + x         + ' eostok')\n",
        "# print(post_code.head(2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    np.array(post_code[\"code\"]), \n",
        "    np.array(post_code[\"summary\"]),\n",
        "    test_size=0.15,\n",
        "    random_state=0,\n",
        "    shuffle=True,\n",
        ")\n",
        "# print(x_test, y_test)\n",
        "\n",
        "\n",
        "############# Preparing code_vocab and summary_vocab using Tokenizer #############################\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "############### Preparing x_vocab or code_vocab\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "\n",
        "threshold = 2\n",
        "cnt_infrequent = 0\n",
        "total_cnt = 0\n",
        "for key, value in x_tokenizer.word_counts.items():\n",
        "    total_cnt = total_cnt + 1\n",
        "    if value < threshold:\n",
        "        cnt_infrequent = cnt_infrequent + 1 \n",
        "print(\"% of not frequent words in vocabulary: \", (cnt_infrequent / total_cnt) * 100)\n",
        "\n",
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "x_tokenizer = Tokenizer(num_words = total_cnt - cnt_infrequent) \n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "# Convert text sequences to integer sequences \n",
        "x_train_seqs = x_tokenizer.texts_to_sequences(x_train) \n",
        "x_test_seqs = x_tokenizer.texts_to_sequences(x_test)\n",
        "print (x_test_seqs)\n",
        "# Pad zero upto maximum length\n",
        "x_train = pad_sequences(x_train_seqs,  maxlen=max_code_len, padding='post')\n",
        "x_test = pad_sequences(x_test_seqs, maxlen=max_code_len, padding='post')\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "x_voc = x_tokenizer.num_words + 1\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))\n",
        "\n",
        "############### Preparing y_vocab or summary_vocab\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "threshold = 2\n",
        "cnt_infrequent = 0\n",
        "total_cnt = 0\n",
        "# max_freq =0\n",
        "for key, value in y_tokenizer.word_counts.items():\n",
        "    total_cnt = total_cnt + 1\n",
        "    # max_freq = max(max_freq, value)\n",
        "    if value < threshold:\n",
        "        cnt_infrequent = cnt_infrequent + 1  \n",
        "print(\"% of rare words in vocabulary:\",(cnt_infrequent / total_cnt) * 100)\n",
        "\n",
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "y_tokenizer = Tokenizer(num_words = total_cnt - cnt_infrequent) \n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "# Convert text sequences to integer sequences \n",
        "y_train_seqs = y_tokenizer.texts_to_sequences(y_train) \n",
        "y_test_seqs = y_tokenizer.texts_to_sequences(y_test)\n",
        "# Pad zero upto maximum length\n",
        "y_train = pad_sequences(y_train_seqs,  maxlen=max_summary_len, padding='post')\n",
        "y_test = pad_sequences(y_test_seqs, maxlen=max_summary_len, padding='post')\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "y_voc = y_tokenizer.num_words + 1\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))\n",
        "\n",
        "print(\"This is X_vocaboulary:-\")\n",
        "print(x_train)\n",
        "print(\"This is Y_vocaboulary:-\")\n",
        "print(y_train)\n",
        "\n",
        "########## remove \"Summary\" i.e Y (both train and val) which has only START and END\n",
        "ind = []\n",
        "for i in range(len(y_train)):\n",
        "    cnt = 0\n",
        "    for j in y_train[i]:\n",
        "        if j != 0:\n",
        "            cnt = cnt + 1\n",
        "    if cnt == 2:\n",
        "        ind.append(i)\n",
        "y_train = np.delete(y_train, ind, axis=0)\n",
        "x_train = np.delete(x_train, ind, axis=0)\n",
        "\n",
        "ind = []\n",
        "for i in range(len(y_test)):\n",
        "    cnt = 0\n",
        "    for j in y_test[i]:\n",
        "        if j != 0:\n",
        "            cnt = cnt + 1\n",
        "    if cnt == 2:\n",
        "        ind.append(i)\n",
        "y_test = np.delete(y_test, ind, axis=0)\n",
        "x_test = np.delete(x_test, ind, axis=0)\n",
        "\n",
        "# print(x_train)\n",
        "# print(y_train)\n",
        "# print(x_test)\n",
        "# print(y_test)\n",
        "print((x_train[0]))\n",
        "print((y_train[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############### LSTM based Encoder-Decoder Architecture and Training  ####################################\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim = 200\n",
        "\n",
        "#######  Encoder Architecture ####################################################\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_code_len, ))\n",
        "# Embedding layer\n",
        "enc_emb = Embedding(x_voc, embedding_dim,\n",
        "trainable=True)(encoder_inputs)\n",
        "# Encoder LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
        "return_state=True, dropout=0.4,\n",
        "recurrent_dropout=0.4)\n",
        "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
        "# Encoder LSTM 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
        "return_state=True, dropout=0.4,\n",
        "recurrent_dropout=0.4)\n",
        "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
        "# Encoder LSTM 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
        "return_sequences=True, dropout=0.4,\n",
        "recurrent_dropout=0.4)\n",
        "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
        "\n",
        "\n",
        "########### Set up the decoder, using encoder_states as the initial state\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "# Embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
        "return_state=True, dropout=0.4,\n",
        "recurrent_dropout=0.2)\n",
        "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
        "decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.summary()\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "print(\"########### 1 (80, 50)============================================================\")\n",
        "history1 = model.fit(\n",
        "    [x_train, y_train[:, :-1]],\n",
        "    y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:, 1:],\n",
        "    epochs=50,\n",
        "    callbacks=[es],\n",
        "    batch_size=128,\n",
        "    validation_data=([x_test, y_test[:, :-1]],y_test.reshape(y_test.shape[0], y_test.shape[1], 1)[:, 1:]),\n",
        "    )\n",
        "\n",
        "# print('val_loss: {}, val_acc : {}'.format(val_loss, val_accuracy))\n",
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.plot(history1.history['loss'], label='train1')\n",
        "pyplot.plot(history1.history['val_loss'], label='test1')\n",
        "pyplot.legend()\n",
        "pyplot.show()\n",
        "\n",
        "reverse_target_word_index = y_tokenizer.index_word\n",
        "reverse_source_word_index = x_tokenizer.index_word\n",
        "target_word_index = y_tokenizer.word_index\n",
        "\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "####### Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
        "decoder_hidden_state_input = Input(shape=(max_code_len, latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
        "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
        "                      decoder_state_input_h, decoder_state_input_c],\n",
        "                      [decoder_outputs2] + [state_h2, state_c2])\n",
        "\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        (output_tokens, h, c) = decoder_model.predict([target_seq]+ [e_out, e_h, e_c])\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        if sampled_token != 'eostok':\n",
        "            decoded_sentence += ' ' + sampled_token\n",
        "        # Exit condition: either hit max length or find the stop word.\n",
        "        if sampled_token == 'eostok' or len(decoded_sentence.split()) >= max_summary_len - 1:\n",
        "            stop_condition = True\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        # Update internal states\n",
        "        (e_h, e_c) = (h, c)\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "\n",
        "def seq2summary(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0 and i != target_word_index['sostok'] and i != target_word_index['eostok']:\n",
        "            newString = newString + reverse_target_word_index[i] + ' '\n",
        "    return newString\n",
        "\n",
        "\n",
        "######### To convert sequence to text\n",
        "def seq2text(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0:\n",
        "            newString = newString + reverse_source_word_index[i] + ' '\n",
        "    return newString\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "sum = 0\n",
        "for i in range(0, 75):\n",
        "    print ('Code:', seq2text(x_train[i]))\n",
        "    reference = seq2summary(y_train[i])\n",
        "    candidate = decode_sequence(x_train[i].reshape(1, max_code_len))\n",
        "    print ('Original summary:', reference)\n",
        "    print ('Predicted summary:', candidate)\n",
        "    score = sentence_bleu(reference, candidate)\n",
        "    sum += score\n",
        "    print('BLEU Score:', score)\n",
        "    print ('\\n')\n",
        "\n",
        "sum /= y_train.shape[0]\n",
        "print('avg_BLEU_Score: ', sum)\n",
        "################# Calculating BLEU Score ###############################\n",
        "\n"
      ]
    }
  ]
}